---
layout: page
title: Undergraduate Thesis
description: 3D Reconstruction of Articulated Objects
img: assets/img/thesis/isaac_mesh.png
# video:
importance: 1
category: learning
---

Digital twins let roboticists train or test manipulation policies entirely in simulation before transferring them back to the real world, making them useful in improving scalability and flexibility in robotics development. For my undergraduate thesis, I survey the current landscape in the field of 3D reconstruction and introduce a pipeline for open-vocabulary, object-level method for generation of an articulated mesh purely from monocular RGB videos. The method leverages a 3d scan of the object in question along with a video demonstration of the object's articulation. The method combines multi-scale semantic segmentation using neural radiance fields (NeRFs) with a 3d Gaussian splatting-based (3DGS) objecting tracking method to create a segmented splat of the object and extract relative poses of each object part. A least-squares hinge-estimation stage then recovers the best-fit revolute joint, allowing the creation of a simulation-ready articulated mesh.

    ❌ no depth camera

    ❌ no training required

    ✅ works with any objects (open-vocabulary)

    ⚠️ manually specify object for crop

    ⚠️ manually specify scale for segmentation

## Important Papers

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid loading="eager" path="assets/video/thesis/render_rgb.mp4" title="example image" class="img-fluid rounded z-depth-1" autoplay="true" loop="true"%}
    </div>
</div>
<div class="caption">
    RGB render of 3d gaussian splatting model of cooler.
</div>

### Group Anything with Radiance Fields (GARField)

![garfield scene]({{ site.baseurl }}/assets/thesis/garfield_scene.png)

[GARField](https://www.garfield.studio) addresses the problem of multi-scale 3D segmentation. (e.g., should a jar and its lid be treated as separate parts or a single whole?). Starting from open-vocabulary 2D masks generated by SAM, GARField trains a NeRF that outputs, in addition to colour and density, a scale-conditioned affinity vector. The affinity allows classifying two 3D points as belonging to the same part or not, based on the cosine distance between the vectors.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/thesis/garfield_scene_segmented.png" title="garfield scene segmented" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/thesis/garfield_parts.png" title="garfield parts" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<!-- <div class="caption">
</div> -->

Pipeline.
1. Mask harvesting. Each training view is densely queried with SAM, yielding overlapping 2D masks of various sizes. The 2d mask scale is converted to a 3d scale using depth from the NeRF.
2. Scale-conditioned training. For every pair of rays in the same image, the network (i) pulls feature vectors together if the rays lie in the same mask and (ii) pushes them apart otherwise, but only at the scale of that mask. Two auxiliary terms densify supervision along the scale axis (continuous sampling) and enforce containment (small-scale groups must persist at larger scales).

![plot of scale-conditioning]({{ site.baseurl }}/assets/)

3. Hierarchy extraction. After convergence, HDBSCAN is run on the affinity field at successively decreasing scales, producing a tree in which parent clusters subdivide into child parts.
Users can stop at any depth or interactively click to retrieve a desired granularity

### Robot See Robot Do

[Robot See, Robot Do](https://robot-see-robot-do.github.io) (RSRD) extends GARField to end-to-end imitation: a dual-arm robot watches a single monocular demonstration and then reproduces the same articulated motion on the physical object. The pipeline assumes two inputs:

1. An RGB video scan of the object
2. An RGB video demonstrating the object's articulation (e.g., opening a cooler lid).

4D Differentiable Part Model (4D-DPM). RSRD first trains a GARField scene on the scan video, then selects the object and an appropriate scale to isolate its parts. Those clusters are distilled into a 3DGS augmented with DINO features, yielding a fully differentiable, feature-rich representation. During the demonstration video, the system per-part SE(3) trajectories so that rendered 3DGS features align with the observed frames, using Adam for fifty iterations per timestep, with additional regularization for smoothness. The result is a trajectory consisting of a per-frame for each part.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid loading="eager" path="assets/video/thesis/render_dino.mp4" title="example image" class="img-fluid rounded z-depth-1" autoplay="true" loop="true"%}
    </div>
</div>
<div class="caption">
    RGB render of 3d gaussian splatting model of cooler.
</div>

Imitation Planning. Given these trajectories, a motion planner synthesises dual-arm motions that realise the same part displacements while respecting the robot’s kinematics. Crucially, the planner targets the object frame rather than the demonstrator’s hand pose, allowing grasps that differ from the human’s yet achieve the same functional outcome. The system is validated using a YuMi bimanual robot with nine everyday objects, achieving an average end-to-end success rate of 60 without task-specific training.

## Extension

After obtaining relative transformations between parts A and B at each time step, we can fit the relative tranformations to a plane using PCA. In the plane, we perform a least-squares circle fit to obtain the pivot point.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/thesis/point_cloud_hinge_0.png" title="garfield scene segmented" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/thesis/point_cloud_hinge_1.png" title="garfield parts" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Visualizing the hinge-fitting process using a point cloud.
</div>

Now, convert the segmented 3dgs models to meshes using Marching Cubes. We can create a final USD file (or MJCF or similar alternative) constituting the final model, containing the complete object, with articulation.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid loading="eager" path="assets/video/thesis/mesh_anim_0.mp4" title="example image" class="img-fluid rounded z-depth-1" autoplay="true" loop="true"%}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid loading="eager" path="assets/video/thesis/mesh_anim_1.mp4" title="example image" class="img-fluid rounded z-depth-1" autoplay="true" loop="true"%}
    </div>
</div>
<div class="caption">
    Visualizing the final articulated mesh in IsaacSim.
</div>

## Future Improvements

### Improved Mesh Reconstruction

Marching Cubes sometimes produces poor mesh reconstructions when extracting from a 3dgs in our tests. This is partially caused by the sparse nature of 3dgs models and results in unwanted holes as well as elliptical artifacts un the mesh's surface. Surface-aligned gaussian splatting for efficient 3d mesh
reconstruction (SuGaR) by Guedon et al. describes a better method for reconstructing higher quality meshes that would complement our pipeline well.

### Enhancing 3d Representation with Diffusion Models

Our 3D gaussian splat representation for object parts are prone to two notable effects reducing quality. First, in regions that have few observations or insufficient visually salient features, the 3dgs model sometimes produces unwanted gaussians that protrude from the objects surface or float near it. Next, on parts of the object that are occluded, for example, due to being in contact with a wall or another part of the object, there are also unwanted floating or protruding gaussians.

The first issue has been examined in works like Difix3D+, by Wu, Zhang et al., which leverages a diffusion model to denoise rendered views and then uses the improved views to refine the underlying 3dgs. This approach would benefit our pipeline. A promising avenue could be to denoise or potentially complete the unseen parts of objects / object parts, resulting in better quality reconstructions.